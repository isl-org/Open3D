{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import open3d as o3d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scalable TSDF integration\n",
    "\n",
    "## TSDF voxels\n",
    "TSDF (truncated signed distance function) is a useful map representation to fuse and denoise a sequence of Depth/RGB-D images with known camera poses. It voxelizes the 3D space; for each voxel, it computes weight average of signed distance to its closest surface observations from multiple scans. Note this signed distance is **truncated** to preserve details, so that voxels too far away from the surface won't be affected.\n",
    "\n",
    "By this description, each voxel stores a `float` tsdf value and a `float` or `uint16` weight. At current, we also optionally support `rgb` colors of `float` or `uint16` (we do not use `uint8` to preserve precision during integration). Supported combinations are:\n",
    "- `float` tsdf, `float` weight\n",
    "- `float` tsdf, `float` weight, `float` rgb\n",
    "- `float` tsdf, `uint16` weight, `uint16` rgb\n",
    "\n",
    "Users may also customize their own voxel types and include other properties (e.g., semantic mask), after modifying the voxels and dispatch pattern in `core/kernel/GeneralEWSharedImpl.h`.\n",
    "\n",
    "## Voxel blocks and spatial hashing\n",
    "Initially, in KinectFusion, TSDF is defined in a restricted $512^3$ cube. While this is useful to represent small scenes, it has to sacrifice reconstruction resolution for larger scale environments. VoxelHashing introduces an improved representation where the 3D space is roughly divided into $16^3$ **voxel blocks**. These voxel blocks will only be allocated around surfaces, i.e., around depth observations, and can be indexed dynamically by 3D coordinate using a **hashmap**.\n",
    "\n",
    "Based on these, we can generate a high resolution volume in such a way:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "voxel_size = 0.008  # voxel resolution in meter\n",
    "sdf_trunc = 0.04  # truncation distance in meter\n",
    "block_resolution = 16  # 16^3 voxel blocks\n",
    "initial_block_count = 1000  # initially allocated number of voxel blocks\n",
    "device = o3d.core.Device('cuda:0')  # or 'cuda:0' if you have cuda support\n",
    "\n",
    "volume = o3d.t.geometry.TSDFVoxelGrid(\n",
    "    {\n",
    "        'tsdf': o3d.core.Dtype.Float32,\n",
    "        'weight': o3d.core.Dtype.UInt16,\n",
    "        'color': o3d.core.Dtype.UInt16\n",
    "    },\n",
    "    voxel_size=voxel_size,\n",
    "    sdf_trunc=sdf_trunc,\n",
    "    block_resolution=block_resolution,\n",
    "    block_count=initial_block_count,\n",
    "    device=device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note this voxel size is very high. For smaller scenes (e.g. `stanford/lounge`, `stanford/copyroom`), it should work fine. But for larger scenes (e.g. `stanford/burghers`, `indoor_lidar_rgbd/apartment`), there can be a memory issue. Here are several workarounds if your machine has a limited memory (especially CUDA memory):\n",
    "- Increase `voxel_size` to 0.01, or a larger value. This will not significantly reduce reconstruction accuracy, but only consumes half the memory.\n",
    "- Pre-estimate `intial_block_count`. While our internal **hashmap** supports automatic resize when the observed scene is growing, its peak memory consumption is large. If you are able to roughly estimate the number of voxels blocks, rehash will not take place, and there will not be a peak memory issue. As a reference `stanford/lounge` required around 30K voxel blocks, and `indoor_lidar_rgbd/apartment` requires around 80K voxel blocks.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input\n",
    "We then prepare the input, including intrinsics, camera trajectories, and rgbd images for integration. The trajectory file is with the `.log` format that can be generated via the reconstruction system."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "intrinsic = o3d.camera.PinholeCameraIntrinsic(\n",
    "    o3d.camera.PinholeCameraIntrinsicParameters.PrimeSenseDefault)\n",
    "\n",
    "intrinsic = o3d.core.Tensor(intrinsic.intrinsic_matrix, o3d.core.Dtype.Float32,\n",
    "                            device)\n",
    "\n",
    "\n",
    "class CameraPose:\n",
    "\n",
    "    def __init__(self, meta, mat):\n",
    "        self.metadata = meta\n",
    "        self.pose = mat\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Metadata : ' + ' '.join(map(str, self.metadata)) + '\\n' + \\\n",
    "            \"Pose : \" + \"\\n\" + np.array_str(self.pose)\n",
    "\n",
    "\n",
    "def read_trajectory(filename):\n",
    "    traj = []\n",
    "    with open(filename, 'r') as f:\n",
    "        metastr = f.readline()\n",
    "        while metastr:\n",
    "            metadata = list(map(int, metastr.split()))\n",
    "            mat = np.zeros(shape=(4, 4))\n",
    "            for i in range(4):\n",
    "                matstr = f.readline()\n",
    "                mat[i, :] = np.fromstring(matstr, dtype=float, sep=' \\t')\n",
    "            traj.append(CameraPose(metadata, mat))\n",
    "            metastr = f.readline()\n",
    "    return traj\n",
    "\n",
    "\n",
    "camera_poses = read_trajectory(\"../../test_data/RGBD/odometry.log\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Integration\n",
    "Finally we start the integration. Note in this newest version, we are shifting to tensor-based representations, so a conversion is required. Here, `depth_scale` controls the conversion from raw depth images to meter metric depths. Depth max constraints the furthest (and usually the most noisy) points we want to integrate."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(len(camera_poses)):\n",
    "    print(\"Integrate {:d}-th image into the volume.\".format(i))\n",
    "    color = o3d.io.read_image(\"../../test_data/RGBD/color/{:05d}.jpg\".format(i))\n",
    "    color = o3d.t.geometry.Image.from_legacy(color, device=device)\n",
    "\n",
    "    depth = o3d.io.read_image(\"../../test_data/RGBD/depth/{:05d}.png\".format(i))\n",
    "    depth = o3d.t.geometry.Image.from_legacy(depth, device=device)\n",
    "\n",
    "    extrinsic = o3d.core.Tensor(np.linalg.inv(camera_poses[i].pose),\n",
    "                                o3d.core.Dtype.Float32, device)\n",
    "    volume.integrate(depth, color, intrinsic, extrinsic, 1000.0, 3.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Surface extraction\n",
    "After TSDF is reconstructed, we can find zero-crossings in the TSDF grid and extract the surfaces. We both support mesh and surface extraction. \n",
    "Note surface extraction directly from CUDA is memory consuming. If you want real-time visualization or surface extraction from relatively small scenes, you can directly run from cuda. Otherwise, you may choose to extract mesh after moving it to `.cpu()`. The below implementations showcase both."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mesh = volume.cpu().extract_surface_mesh().to_legacy()\n",
    "mesh.compute_vertex_normals()\n",
    "o3d.visualization.draw_geometries([mesh],\n",
    "                                  front=[0.5297, -0.1873, -0.8272],\n",
    "                                  lookat=[2.0712, 2.0312, 1.7251],\n",
    "                                  up=[-0.0558, -0.9809, 0.1864],\n",
    "                                  zoom=0.47)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pcd = volume.extract_surface_points().to_legacy()\n",
    "o3d.visualization.draw_geometries([pcd],\n",
    "                                  front=[0.5297, -0.1873, -0.8272],\n",
    "                                  lookat=[2.0712, 2.0312, 1.7251],\n",
    "                                  up=[-0.0558, -0.9809, 0.1864],\n",
    "                                  zoom=0.47)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}